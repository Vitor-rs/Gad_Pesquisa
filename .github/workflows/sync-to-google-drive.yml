name: Sync PDFs to Google Drive (Optimized)

on:
  push:
    branches: [main, master]
    paths: ['**/*.pdf']  # Trigger apenas para PDFs
  workflow_dispatch:

jobs:
  sync-pdfs:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Timeout estendido para PDFs
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1  # Shallow clone para performance
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install Dependencies
      run: |
        pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2
        pip install tqdm  # Para progress bars
    
    - name: Optimized PDF Sync
      env:
        GOOGLE_CLIENT_ID: ${{ secrets.GOOGLE_CLIENT_ID }}
        GOOGLE_CLIENT_SECRET: ${{ secrets.GOOGLE_CLIENT_SECRET }}
        GOOGLE_REFRESH_TOKEN: ${{ secrets.GOOGLE_REFRESH_TOKEN }}
        GOOGLE_DRIVE_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
      run: |
        python << 'EOF'
        import os
        import json
        import hashlib
        from pathlib import Path
        from datetime import datetime
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from google.auth.transport.requests import Request
        from google.oauth2.credentials import Credentials
        from googleapiclient.discovery import build
        from googleapiclient.http import MediaFileUpload
        from googleapiclient.errors import HttpError
        from tqdm import tqdm
        import time
        
        class OptimizedPDFSync:
            def __init__(self):
                self.service = self._authenticate()
                self.folder_id = os.environ['GOOGLE_DRIVE_FOLDER_ID']
                self.repo_name = os.environ.get('GITHUB_REPOSITORY', '').split('/')[-1]
                self.batch_size = 10  # Processar em lotes
                self.max_workers = 3  # Threads concorrentes limitadas
                
            def _authenticate(self):
                """Autentica√ß√£o otimizada com refresh token"""
                creds = Credentials(
                    token=None,
                    refresh_token=os.environ['GOOGLE_REFRESH_TOKEN'],
                    token_uri='https://oauth2.googleapis.com/token',
                    client_id=os.environ['GOOGLE_CLIENT_ID'],
                    client_secret=os.environ['GOOGLE_CLIENT_SECRET']
                )
                creds.refresh(Request())
                return build('drive', 'v3', credentials=creds)
            
            def _calculate_file_md5(self, filepath):
                """Calcula MD5 para verifica√ß√£o de integridade"""
                hash_md5 = hashlib.md5()
                with open(filepath, "rb") as f:
                    for chunk in iter(lambda: f.read(4096), b""):
                        hash_md5.update(chunk)
                return hash_md5.hexdigest()
            
            def _get_existing_files(self):
                """Lista arquivos existentes no Drive para evitar duplicatas"""
                try:
                    query = f"'{self.folder_id}' in parents and trashed=false"
                    results = self.service.files().list(
                        q=query,
                        fields="files(id, name, md5Checksum, size)",
                        pageSize=1000
                    ).execute()
                    
                    existing_files = {}
                    for file in results.get('files', []):
                        existing_files[file['name']] = {
                            'id': file['id'],
                            'md5': file.get('md5Checksum'),
                            'size': file.get('size')
                        }
                    return existing_files
                except HttpError as e:
                    print(f"Erro ao listar arquivos existentes: {e}")
                    return {}
            
            def _needs_update(self, local_path, existing_file_info):
                """Verifica se arquivo precisa ser atualizado"""
                if not existing_file_info:
                    return True
                
                local_size = os.path.getsize(local_path)
                remote_size = int(existing_file_info.get('size', 0))
                
                if local_size != remote_size:
                    return True
                
                # Para arquivos grandes, verificar MD5
                if local_size > 5 * 1024 * 1024:  # > 5MB
                    local_md5 = self._calculate_file_md5(local_path)
                    return local_md5 != existing_file_info.get('md5')
                
                return False
            
            def _upload_file_chunked(self, filepath, filename, existing_file_id=None):
                """Upload otimizado com chunks para arquivos grandes"""
                try:
                    file_size = os.path.getsize(filepath)
                    
                    # Configurar upload baseado no tamanho
                    if file_size > 10 * 1024 * 1024:  # > 10MB
                        chunk_size = 1024 * 1024  # 1MB chunks
                        resumable = True
                    else:
                        chunk_size = -1  # Upload completo
                        resumable = False
                    
                    media = MediaFileUpload(
                        filepath,
                        mimetype='application/pdf',
                        chunksize=chunk_size,
                        resumable=resumable
                    )
                    
                    if existing_file_id:
                        # Atualizar arquivo existente
                        request = self.service.files().update(
                            fileId=existing_file_id,
                            media_body=media
                        )
                    else:
                        # Criar novo arquivo
                        file_metadata = {
                            'name': filename,
                            'parents': [self.folder_id]
                        }
                        request = self.service.files().create(
                            body=file_metadata,
                            media_body=media,
                            fields='id'
                        )
                    
                    # Execute com retry autom√°tico
                    response = None
                    for attempt in range(3):
                        try:
                            if resumable:
                                response = None
                                while response is None:
                                    status, response = request.next_chunk()
                                    if status:
                                        progress = int(status.progress() * 100)
                                        print(f"  ‚îî‚îÄ Upload progress: {progress}%")
                            else:
                                response = request.execute()
                            break
                        except HttpError as e:
                            if attempt == 2:  # √öltima tentativa
                                raise e
                            print(f"  ‚ö†Ô∏è  Tentativa {attempt + 1} falhou, retry em 5s...")
                            time.sleep(5)
                    
                    return response.get('id') if response else None
                    
                except Exception as e:
                    print(f"  ‚ùå Erro no upload de {filename}: {e}")
                    return None
            
            def _process_pdf_batch(self, pdf_files_batch, existing_files):
                """Processa um lote de PDFs"""
                results = []
                
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_file = {}
                    
                    for pdf_path in pdf_files_batch:
                        filename = pdf_path.name
                        existing_info = existing_files.get(filename)
                        
                        if not self._needs_update(pdf_path, existing_info):
                            print(f"  ‚è≠Ô∏è  {filename} j√° est√° atualizado")
                            continue
                        
                        existing_id = existing_info['id'] if existing_info else None
                        future = executor.submit(
                            self._upload_file_chunked, 
                            pdf_path, 
                            filename, 
                            existing_id
                        )
                        future_to_file[future] = (pdf_path, filename)
                    
                    for future in as_completed(future_to_file):
                        pdf_path, filename = future_to_file[future]
                        try:
                            file_id = future.result()
                            if file_id:
                                action = "Atualizado" if filename in existing_files else "Criado"
                                print(f"  ‚úÖ {action}: {filename}")
                                results.append((filename, "success"))
                            else:
                                results.append((filename, "failed"))
                        except Exception as e:
                            print(f"  ‚ùå Falha em {filename}: {e}")
                            results.append((filename, "error"))
                
                return results
            
            def sync_pdfs(self):
                """M√©todo principal de sincroniza√ß√£o otimizada"""
                print("üîÑ Iniciando sincroniza√ß√£o otimizada de PDFs...")
                
                # Encontrar todos os PDFs
                pdf_files = list(Path('.').rglob('*.pdf'))
                if not pdf_files:
                    print("üìÅ Nenhum arquivo PDF encontrado no reposit√≥rio")
                    return
                
                print(f"üìä Encontrados {len(pdf_files)} arquivos PDF")
                
                # Buscar arquivos existentes no Drive
                print("üîç Verificando arquivos existentes no Drive...")
                existing_files = self._get_existing_files()
                print(f"üìã {len(existing_files)} arquivos encontrados no Drive")
                
                # Processar em lotes
                total_processed = 0
                total_success = 0
                
                for i in range(0, len(pdf_files), self.batch_size):
                    batch = pdf_files[i:i + self.batch_size]
                    batch_num = (i // self.batch_size) + 1
                    total_batches = (len(pdf_files) + self.batch_size - 1) // self.batch_size
                    
                    print(f"\nüì¶ Processando lote {batch_num}/{total_batches} ({len(batch)} arquivos)")
                    
                    results = self._process_pdf_batch(batch, existing_files)
                    
                    batch_success = sum(1 for _, status in results if status == "success")
                    total_processed += len(results)
                    total_success += batch_success
                    
                    print(f"  üìà Lote {batch_num}: {batch_success}/{len(batch)} sucessos")
                    
                    # Pausa entre lotes para evitar rate limiting
                    if i + self.batch_size < len(pdf_files):
                        time.sleep(2)
                
                # Resumo final
                print(f"\n‚ú® Sincroniza√ß√£o conclu√≠da!")
                print(f"üìä Estat√≠sticas:")
                print(f"  ‚Ä¢ Total de arquivos: {len(pdf_files)}")
                print(f"  ‚Ä¢ Processados: {total_processed}")
                print(f"  ‚Ä¢ Sucessos: {total_success}")
                print(f"  ‚Ä¢ Taxa de sucesso: {(total_success/max(total_processed,1)*100):.1f}%")
        
        # Executar sincroniza√ß√£o
        if __name__ == "__main__":
            syncer = OptimizedPDFSync()
            syncer.sync_pdfs()
        
        EOF
