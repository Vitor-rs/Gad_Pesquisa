name: Bidirectional Sync GitHub ‚Üî Google Drive

on:
  push:
    branches: [ main, master ]
  schedule:
    # Executar a cada 30 minutos (aumentado para dar tempo de processar muitos arquivos)
    - cron: '*/30 * * * *'
  workflow_dispatch:
    inputs:
      sync_mode:
        description: 'Modo de sincroniza√ß√£o'
        required: false
        default: 'bidirectional'
        type: choice
        options:
          - bidirectional
          - drive-to-github
          - github-to-drive
      force_sync:
        description: 'For√ßar sincroniza√ß√£o completa (ignorar cache)'
        required: false
        default: false
        type: boolean

jobs:
  sync-bidirectional:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 horas para grandes volumes
    
    # Pula apenas se o commit anterior foi feito pelo pr√≥prio bot
    if: |
      github.event_name == 'schedule' || 
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'push' && !contains(github.event.head_commit.message, '[sync-bot]'))
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
        lfs: true  # Suporte para arquivos grandes
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Configure Git LFS
      run: |
        git lfs install
        git config --global lfs.concurrenttransfers 10
        
    - name: Bidirectional Sync
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        SYNC_MODE: ${{ github.event.inputs.sync_mode || 'bidirectional' }}
        FORCE_SYNC: ${{ github.event.inputs.force_sync || 'false' }}
      run: |
        pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 requests python-dateutil tqdm
        
        python << 'EOF'
        import os
        import sys
        import json
        import hashlib
        import requests
        import subprocess
        import time
        from datetime import datetime, timezone, timedelta
        from dateutil import parser
        from pathlib import Path
        from googleapiclient.discovery import build
        from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
        from google.oauth2.credentials import Credentials
        from googleapiclient.errors import HttpError
        import io
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from threading import Lock
        import traceback
        
        print("üîÑ SINCRONIZA√á√ÉO BIDIRECIONAL GITHUB ‚Üî GOOGLE DRIVE")
        print("=" * 60)
        
        # Configura√ß√µes
        repo_name = 'Gad_Pesquisa'
        branch = 'main'
        folder_id = '${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}'
        github_username = 'Vitor-rs'
        github_email = 'vitor.santos9@estudante.ifms.edu.br'
        
        # Credenciais OAuth2
        client_id = '${{ secrets.GOOGLE_CLIENT_ID }}'
        client_secret = '${{ secrets.GOOGLE_CLIENT_SECRET }}'
        refresh_token = '${{ secrets.GOOGLE_REFRESH_TOKEN }}'
        
        # Configura√ß√µes de sincroniza√ß√£o
        BATCH_SIZE = 50  # Processar arquivos em lotes
        MAX_WORKERS = 5  # Threads paralelas para download/upload
        MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB - limite do GitHub
        CHUNK_SIZE = 10 * 1024 * 1024  # 10MB chunks para uploads grandes
        
        # Arquivos de controle
        SYNC_STATE_FILE = '.sync_state.json'
        SYNC_HISTORY_FILE = '_SYNC_HISTORY.md'
        SYNC_LOCK_FILE = '.sync.lock'
        
        # Modo de sincroniza√ß√£o
        SYNC_MODE = os.environ.get('SYNC_MODE', 'bidirectional')
        FORCE_SYNC = os.environ.get('FORCE_SYNC', 'false').lower() == 'true'
        
        print(f"üìã Modo: {SYNC_MODE}")
        print(f"üîÑ For√ßar sincroniza√ß√£o: {FORCE_SYNC}")
        
        # Verificar credenciais
        missing_secrets = []
        if not client_id: missing_secrets.append('GOOGLE_CLIENT_ID')
        if not client_secret: missing_secrets.append('GOOGLE_CLIENT_SECRET')
        if not refresh_token: missing_secrets.append('GOOGLE_REFRESH_TOKEN')
        if not folder_id: missing_secrets.append('GOOGLE_DRIVE_FOLDER_ID')
        
        if missing_secrets:
            print(f"‚ùå ERRO: Secrets n√£o configurados: {', '.join(missing_secrets)}")
            sys.exit(1)
        
        # Criar lock para evitar execu√ß√µes concorrentes
        if os.path.exists(SYNC_LOCK_FILE):
            with open(SYNC_LOCK_FILE, 'r') as f:
                lock_data = json.load(f)
                lock_time = datetime.fromisoformat(lock_data['timestamp'])
                if datetime.now(timezone.utc) - lock_time < timedelta(hours=1):
                    print("‚ö†Ô∏è Sincroniza√ß√£o j√° em andamento (lock ativo)")
                    sys.exit(0)
        
        # Criar novo lock
        with open(SYNC_LOCK_FILE, 'w') as f:
            json.dump({'timestamp': datetime.now(timezone.utc).isoformat()}, f)
        
        try:
            # Renovar access token
            print("üîÑ Renovando access token...")
            token_url = "https://oauth2.googleapis.com/token"
            token_data = {
                'client_id': client_id,
                'client_secret': client_secret,
                'refresh_token': refresh_token,
                'grant_type': 'refresh_token'
            }
            
            response = requests.post(token_url, data=token_data)
            if response.status_code != 200:
                print(f"‚ùå Erro ao renovar token: {response.text}")
                sys.exit(1)
            
            access_token = response.json()['access_token']
            print("‚úÖ Token renovado com sucesso!")
            
            # Criar credenciais e servi√ßo
            credentials = Credentials(
                token=access_token,
                refresh_token=refresh_token,
                token_uri="https://oauth2.googleapis.com/token",
                client_id=client_id,
                client_secret=client_secret,
                scopes=['https://www.googleapis.com/auth/drive']
            )
            
            service = build('drive', 'v3', credentials=credentials)
            
            # Fun√ß√µes auxiliares
            def calculate_file_hash(file_path):
                """Calcula hash MD5 de um arquivo"""
                if not os.path.exists(file_path):
                    return None
                    
                hash_md5 = hashlib.md5()
                try:
                    with open(file_path, "rb") as f:
                        for chunk in iter(lambda: f.read(8192), b""):
                            hash_md5.update(chunk)
                    return hash_md5.hexdigest()
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao calcular hash de {file_path}: {e}")
                    return None
            
            def get_drive_file_metadata(file_id):
                """Obt√©m metadados completos de um arquivo no Drive"""
                try:
                    return service.files().get(
                        fileId=file_id, 
                        fields='id,name,md5Checksum,size,modifiedTime,mimeType'
                    ).execute()
                except HttpError as e:
                    if e.resp.status == 404:
                        return None
                    raise
            
            def load_sync_state():
                """Carrega estado anterior da sincroniza√ß√£o"""
                if os.path.exists(SYNC_STATE_FILE) and not FORCE_SYNC:
                    try:
                        with open(SYNC_STATE_FILE, 'r') as f:
                            return json.load(f)
                    except:
                        pass
                return {'files': {}, 'last_sync': None, 'version': '2.0'}
            
            def save_sync_state(state):
                """Salva estado atual da sincroniza√ß√£o"""
                state['last_sync'] = datetime.now(timezone.utc).isoformat()
                state['version'] = '2.0'
                with open(SYNC_STATE_FILE, 'w') as f:
                    json.dump(state, f, indent=2)
            
            def should_ignore_file(file_path):
                """Verifica se arquivo deve ser ignorado"""
                ignore_patterns = [
                    '.git/', '__pycache__/', 'node_modules/', '.github/',
                    '.DS_Store', 'Thumbs.db', '~$', '.tmp', '.cache',
                    SYNC_STATE_FILE, SYNC_LOCK_FILE, '.sync_state.json.backup'
                ]
                
                # Ignorar arquivos muito grandes
                if os.path.exists(file_path) and os.path.getsize(file_path) > MAX_FILE_SIZE:
                    return True
                    
                str_path = str(file_path)
                return any(pattern in str_path for pattern in ignore_patterns)
            
            def get_local_files():
                """Obt√©m lista de arquivos locais com hashes"""
                local_files = {}
                total_size = 0
                
                print("üìÇ Escaneando arquivos locais...")
                for path in Path('.').rglob('*'):
                    if path.is_file() and not should_ignore_file(path):
                        rel_path = str(path).replace('\\', '/')
                        try:
                            stat = path.stat()
                            size = stat.st_size
                            total_size += size
                            
                            # Calcular hash apenas para arquivos pequenos por enquanto
                            file_hash = None
                            if size < 10 * 1024 * 1024:  # 10MB
                                file_hash = calculate_file_hash(path)
                            
                            local_files[rel_path] = {
                                'path': rel_path,
                                'hash': file_hash,
                                'size': size,
                                'modified': datetime.fromtimestamp(stat.st_mtime, timezone.utc).isoformat()
                            }
                        except Exception as e:
                            print(f"‚ö†Ô∏è Erro ao processar {rel_path}: {e}")
                
                print(f"  ‚úÖ {len(local_files)} arquivos locais ({total_size / 1024 / 1024:.1f} MB)")
                return local_files
            
            def get_drive_files(parent_id, path='', page_token=None, accumulated_files=None):
                """Obt√©m lista de arquivos do Drive recursivamente com pagina√ß√£o"""
                if accumulated_files is None:
                    accumulated_files = {}
                
                try:
                    # Listar arquivos com pagina√ß√£o
                    query = f"'{parent_id}' in parents and trashed=false"
                    
                    request = service.files().list(
                        q=query,
                        fields='nextPageToken, files(id, name, mimeType, md5Checksum, size, modifiedTime)',
                        pageSize=1000,
                        pageToken=page_token
                    )
                    
                    results = request.execute()
                    files = results.get('files', [])
                    next_page_token = results.get('nextPageToken')
                    
                    for file in files:
                        if file['mimeType'] == 'application/vnd.google-apps.folder':
                            # Processar subpastas recursivamente
                            subfolder_path = f"{path}/{file['name']}" if path else file['name']
                            print(f"  üìÅ Escaneando pasta: {subfolder_path}")
                            get_drive_files(file['id'], subfolder_path, None, accumulated_files)
                        else:
                            # Ignorar Google Docs nativos
                            if file['mimeType'].startswith('application/vnd.google-apps'):
                                continue
                            
                            file_path = f"{path}/{file['name']}" if path else file['name']
                            if not should_ignore_file(file_path):
                                accumulated_files[file_path] = {
                                    'id': file['id'],
                                    'path': file_path,
                                    'hash': file.get('md5Checksum', ''),
                                    'size': int(file.get('size', 0)),
                                    'modified': file.get('modifiedTime', ''),
                                    'mimeType': file.get('mimeType', '')
                                }
                    
                    # Processar pr√≥xima p√°gina se existir
                    if next_page_token:
                        get_drive_files(parent_id, path, next_page_token, accumulated_files)
                    
                except HttpError as e:
                    print(f"‚ö†Ô∏è Erro ao listar arquivos do Drive: {e}")
                
                return accumulated_files
            
            # Thread-safe counters
            download_lock = Lock()
            upload_lock = Lock()
            counters = {'downloaded': 0, 'uploaded': 0, 'errors': 0}
            
            def download_file_with_retry(file_id, local_path, file_size=0):
                """Baixa arquivo do Drive com retry e resume"""
                max_retries = 3
                
                for attempt in range(max_retries):
                    try:
                        os.makedirs(os.path.dirname(local_path), exist_ok=True)
                        
                        request = service.files().get_media(fileId=file_id)
                        
                        # Se arquivo j√° existe parcialmente, fazer resume
                        existing_size = 0
                        if os.path.exists(local_path):
                            existing_size = os.path.getsize(local_path)
                            if existing_size >= file_size:
                                print(f"  ‚úÖ {local_path} j√° existe completo")
                                return True
                            
                            # Resume download
                            request.headers['Range'] = f'bytes={existing_size}-'
                        
                        with open(local_path, 'ab' if existing_size > 0 else 'wb') as f:
                            downloader = MediaIoBaseDownload(f, request, chunksize=CHUNK_SIZE)
                            done = False
                            
                            while not done:
                                status, done = downloader.next_chunk()
                                if status and file_size > CHUNK_SIZE:
                                    progress = (existing_size + status.resumable_progress) / file_size * 100
                                    print(f"  üì• {local_path}: {progress:.1f}%", end='\r')
                        
                        with download_lock:
                            counters['downloaded'] += 1
                        
                        print(f"  ‚úÖ Baixado: {local_path}")
                        return True
                        
                    except Exception as e:
                        print(f"  ‚ùå Erro ao baixar {local_path} (tentativa {attempt + 1}): {e}")
                        if attempt == max_retries - 1:
                            with download_lock:
                                counters['errors'] += 1
                            return False
                        time.sleep(2 ** attempt)  # Backoff exponencial
            
            def upload_file_with_retry(local_path, parent_id, file_id=None):
                """Faz upload de arquivo com retry"""
                max_retries = 3
                
                for attempt in range(max_retries):
                    try:
                        file_metadata = {'name': os.path.basename(local_path)}
                        if not file_id:
                            file_metadata['parents'] = [parent_id]
                        
                        # Para arquivos grandes, usar resumable upload
                        file_size = os.path.getsize(local_path)
                        media = MediaFileUpload(
                            local_path, 
                            resumable=True if file_size > CHUNK_SIZE else False,
                            chunksize=CHUNK_SIZE
                        )
                        
                        if file_id:
                            # Atualizar arquivo existente
                            response = service.files().update(
                                fileId=file_id,
                                media_body=media
                            ).execute()
                        else:
                            # Criar novo arquivo
                            response = service.files().create(
                                body=file_metadata,
                                media_body=media,
                                fields='id'
                            ).execute()
                        
                        with upload_lock:
                            counters['uploaded'] += 1
                        
                        print(f"  ‚úÖ Enviado: {local_path}")
                        return response
                        
                    except Exception as e:
                        print(f"  ‚ùå Erro ao enviar {local_path} (tentativa {attempt + 1}): {e}")
                        if attempt == max_retries - 1:
                            with upload_lock:
                                counters['errors'] += 1
                            return None
                        time.sleep(2 ** attempt)
            
            def ensure_drive_folder_structure(path, parent_id):
                """Garante que a estrutura de pastas existe no Drive"""
                parts = path.split('/')
                current_parent = parent_id
                
                for i, part in enumerate(parts[:-1]):
                    folder_path = '/'.join(parts[:i+1])
                    
                    # Verificar se pasta existe
                    query = f"name='{part}' and '{current_parent}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false"
                    results = service.files().list(q=query, fields='files(id)', pageSize=1).execute()
                    
                    if results.get('files'):
                        current_parent = results['files'][0]['id']
                    else:
                        # Criar pasta
                        folder_metadata = {
                            'name': part,
                            'mimeType': 'application/vnd.google-apps.folder',
                            'parents': [current_parent]
                        }
                        folder = service.files().create(body=folder_metadata, fields='id').execute()
                        current_parent = folder['id']
                        print(f"  üìÅ Pasta criada: {folder_path}")
                
                return current_parent
            
            # Verificar pasta do reposit√≥rio no Drive
            print(f"\nüîç Verificando pasta do reposit√≥rio: {repo_name}")
            query = f"name='{repo_name}' and '{folder_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false"
            results = service.files().list(q=query, fields='files(id)', pageSize=1).execute()
            
            if results.get('files'):
                repo_folder_id = results['files'][0]['id']
                print(f"‚úÖ Pasta encontrada: {repo_name}")
            else:
                # Criar pasta do reposit√≥rio
                folder_metadata = {
                    'name': repo_name,
                    'mimeType': 'application/vnd.google-apps.folder',
                    'parents': [folder_id]
                }
                folder = service.files().create(body=folder_metadata, fields='id').execute()
                repo_folder_id = folder['id']
                print(f"‚úÖ Pasta criada: {repo_name}")
            
            # Carregar estado anterior
            sync_state = load_sync_state()
            previous_files = sync_state.get('files', {})
            
            # Obter arquivos atuais
            print("\nüìä Analisando arquivos...")
            local_files = get_local_files() if SYNC_MODE != 'drive-to-github' else {}
            
            print(f"\n‚òÅÔ∏è Escaneando Google Drive...")
            drive_files = get_drive_files(repo_folder_id) if SYNC_MODE != 'github-to-drive' else {}
            total_drive_size = sum(f['size'] for f in drive_files.values())
            print(f"  ‚úÖ {len(drive_files)} arquivos no Drive ({total_drive_size / 1024 / 1024:.1f} MB)")
            
            # Detectar mudan√ßas
            changes = {
                'local_to_drive': [],  
                'drive_to_local': [],  
                'conflicts': [],       
                'delete_from_drive': [],  
                'delete_from_local': []   
            }
            
            # Analisar mudan√ßas baseado no modo
            if SYNC_MODE == 'drive-to-github':
                # Apenas Drive ‚Üí GitHub
                for path, drive_file in drive_files.items():
                    if path not in local_files:
                        changes['drive_to_local'].append(path)
                    elif not local_files[path]['hash'] or local_files[path]['hash'] != drive_file['hash']:
                        changes['drive_to_local'].append(path)
                
                # Arquivos deletados do Drive
                for path in local_files:
                    if path not in drive_files and path in previous_files:
                        if previous_files[path].get('drive_hash'):
                            changes['delete_from_local'].append(path)
                            
            elif SYNC_MODE == 'github-to-drive':
                # Apenas GitHub ‚Üí Drive
                for path, local_file in local_files.items():
                    if path not in drive_files:
                        changes['local_to_drive'].append(path)
                    elif local_file['hash'] and local_file['hash'] != drive_files[path]['hash']:
                        changes['local_to_drive'].append(path)
                
                # Arquivos deletados localmente
                for path in drive_files:
                    if path not in local_files and path in previous_files:
                        if previous_files[path].get('local_hash'):
                            changes['delete_from_drive'].append(path)
                            
            else:
                # Modo bidirecional
                all_paths = set(local_files.keys()) | set(drive_files.keys()) | set(previous_files.keys())
                
                for path in all_paths:
                    in_local = path in local_files
                    in_drive = path in drive_files
                    in_previous = path in previous_files
                    
                    if in_local and in_drive:
                        # Arquivo existe em ambos
                        local_hash = local_files[path]['hash']
                        drive_hash = drive_files[path]['hash']
                        
                        # Se n√£o temos hash local (arquivo grande), comparar por tamanho e data
                        if not local_hash:
                            local_size = local_files[path]['size']
                            drive_size = drive_files[path]['size']
                            
                            if local_size != drive_size:
                                # Tamanhos diferentes, usar data de modifica√ß√£o
                                local_modified = parser.parse(local_files[path]['modified'])
                                drive_modified = parser.parse(drive_files[path]['modified'])
                                
                                if local_modified > drive_modified:
                                    changes['local_to_drive'].append(path)
                                else:
                                    changes['drive_to_local'].append(path)
                        elif local_hash != drive_hash:
                            # Hashes diferentes
                            if in_previous:
                                prev = previous_files[path]
                                if local_hash != prev.get('local_hash', ''):
                                    if drive_hash != prev.get('drive_hash', ''):
                                        # Conflito
                                        changes['conflicts'].append(path)
                                    else:
                                        changes['local_to_drive'].append(path)
                                elif drive_hash != prev.get('drive_hash', ''):
                                    changes['drive_to_local'].append(path)
                            else:
                                # Sem hist√≥rico, usar data
                                local_modified = parser.parse(local_files[path]['modified'])
                                drive_modified = parser.parse(drive_files[path]['modified'])
                                
                                if local_modified > drive_modified:
                                    changes['local_to_drive'].append(path)
                                else:
                                    changes['drive_to_local'].append(path)
                    
                    elif in_local and not in_drive:
                        if in_previous and previous_files[path].get('drive_hash'):
                            changes['delete_from_local'].append(path)
                        else:
                            changes['local_to_drive'].append(path)
                    
                    elif in_drive and not in_local:
                        if in_previous and previous_files[path].get('local_hash'):
                            changes['delete_from_drive'].append(path)
                        else:
                            changes['drive_to_local'].append(path)
            
            # Resolver conflitos
            for path in changes['conflicts']:
                local_modified = parser.parse(local_files[path]['modified'])
                drive_modified = parser.parse(drive_files[path]['modified'])
                
                if local_modified > drive_modified:
                    changes['local_to_drive'].append(path)
                    print(f"  ‚ö†Ô∏è Conflito em {path}: usando vers√£o local (mais recente)")
                else:
                    changes['drive_to_local'].append(path)
                    print(f"  ‚ö†Ô∏è Conflito em {path}: usando vers√£o do Drive (mais recente)")
            
            # Mostrar resumo
            print("\nüìã Mudan√ßas detectadas:")
            print(f"  ‚¨ÜÔ∏è Local ‚Üí Drive: {len(changes['local_to_drive'])}")
            print(f"  ‚¨áÔ∏è Drive ‚Üí Local: {len(changes['drive_to_local'])}")
            print(f"  ‚ö†Ô∏è Conflitos: {len(changes['conflicts'])}")
            print(f"  üóëÔ∏è Deletar do Drive: {len(changes['delete_from_drive'])}")
            print(f"  üóëÔ∏è Deletar local: {len(changes['delete_from_local'])}")
            
            total_changes = sum(len(changes[k]) for k in changes if k != 'conflicts')
            
            if total_changes == 0:
                print("\n‚úÖ Nenhuma mudan√ßa necess√°ria - tudo sincronizado!")
            else:
                print(f"\nüîÑ Executando {total_changes} opera√ß√µes de sincroniza√ß√£o...")
                
                # Processar em lotes com threads
                start_time = time.time()
                
                # 1. Downloads do Drive (mais importante para seu caso)
                if changes['drive_to_local']:
                    print(f"\n‚¨áÔ∏è Baixando {len(changes['drive_to_local'])} arquivos do Drive...")
                    
                    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                        futures = []
                        
                        for i in range(0, len(changes['drive_to_local']), BATCH_SIZE):
                            batch = changes['drive_to_local'][i:i + BATCH_SIZE]
                            print(f"\nüì¶ Processando lote {i//BATCH_SIZE + 1}/{(len(changes['drive_to_local']) + BATCH_SIZE - 1)//BATCH_SIZE}")
                            
                            for path in batch:
                                if path in drive_files:
                                    future = executor.submit(
                                        download_file_with_retry,
                                        drive_files[path]['id'],
                                        path,
                                        drive_files[path]['size']
                                    )
                                    futures.append((future, path))
                            
                            # Aguardar conclus√£o do lote
                            for future, path in futures:
                                try:
                                    future.result(timeout=300)  # 5 min timeout por arquivo
                                except Exception as e:
                                    print(f"  ‚ùå Erro no download de {path}: {e}")
                            
                            futures.clear()
                    
                    print(f"\n‚úÖ Downloads conclu√≠dos: {counters['downloaded']} sucesso, {counters['errors']} erros")
                
                # 2. Uploads para o Drive
                if changes['local_to_drive']:
                    print(f"\n‚¨ÜÔ∏è Enviando {len(changes['local_to_drive'])} arquivos para o Drive...")
                    
                    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                        futures = []
                        
                        for i in range(0, len(changes['local_to_drive']), BATCH_SIZE):
                            batch = changes['local_to_drive'][i:i + BATCH_SIZE]
                            print(f"\nüì¶ Processando lote {i//BATCH_SIZE + 1}/{(len(changes['local_to_drive']) + BATCH_SIZE - 1)//BATCH_SIZE}")
                            
                            for path in batch:
                                if os.path.exists(path):
                                    parent_id = ensure_drive_folder_structure(path, repo_folder_id)
                                    file_id = drive_files.get(path, {}).get('id')
                                    
                                    future = executor.submit(
                                        upload_file_with_retry,
                                        path,
                                        parent_id,
                                        file_id
                                    )
                                    futures.append((future, path))
                            
                            # Aguardar conclus√£o do lote
                            for future, path in futures:
                                try:
                                    future.result(timeout=300)
                                except Exception as e:
                                    print(f"  ‚ùå Erro no upload de {path}: {e}")
                            
                            futures.clear()
                    
                    print(f"\n‚úÖ Uploads conclu√≠dos: {counters['uploaded']} sucesso, {counters['errors']} erros")
                
                # 3. Deletar arquivos
                for path in changes['delete_from_drive']:
                    if path in drive_files:
                        try:
                            service.files().delete(fileId=drive_files[path]['id']).execute()
                            print(f"  üóëÔ∏è Deletado do Drive: {path}")
                        except Exception as e:
                            print(f"  ‚ùå Erro ao deletar {path}: {e}")
                
                for path in changes['delete_from_local']:
                    try:
                        if os.path.exists(path):
                            os.remove(path)
                            print(f"  üóëÔ∏è Deletado local: {path}")
                    except Exception as e:
                        print(f"  ‚ùå Erro ao deletar {path}: {e}")
                
                elapsed_time = time.time() - start_time
                print(f"\n‚è±Ô∏è Tempo total: {elapsed_time/60:.1f} minutos")
                
                # Criar commit se houver mudan√ßas locais
                if changes['drive_to_local'] or changes['delete_from_local']:
                    print("\nüìù Criando commit com mudan√ßas do Drive...")
                    
                    # Configurar git
                    subprocess.run(['git', 'config', 'user.name', 'Sync Bot'], check=True)
                    subprocess.run(['git', 'config', 'user.email', 'sync-bot@gad-pesquisa.bot'], check=True)
                    
                    # Adicionar mudan√ßas
                    subprocess.run(['git', 'add', '-A'], check=True)
                    
                    # Verificar se h√° mudan√ßas para commitar
                    result = subprocess.run(['git', 'status', '--porcelain'], capture_output=True, text=True)
                    if result.stdout.strip():
                        # Criar mensagem detalhada
                        msg_parts = []
                        if changes['drive_to_local']:
                            msg_parts.append(f"{len(changes['drive_to_local'])} arquivo(s) do Drive")
                        if changes['delete_from_local']:
                            msg_parts.append(f"{len(changes['delete_from_local'])} arquivo(s) removido(s)")
                        
                        commit_message = f"[sync-bot] Sincroniza√ß√£o: {', '.join(msg_parts)}"
                        
                        # Adicionar detalhes ao corpo do commit
                        commit_body = "\n\nDetalhes da sincroniza√ß√£o:\n"
                        if len(changes['drive_to_local']) <= 20:
                            commit_body += "\nArquivos baixados do Drive:\n"
                            for path in sorted(changes['drive_to_local'])[:20]:
                                commit_body += f"- {path}\n"
                        else:
                            commit_body += f"\n{len(changes['drive_to_local'])} arquivos baixados do Drive\n"
                        
                        full_message = commit_message + commit_body
                        
                        # Fazer commit
                        result = subprocess.run(['git', 'commit', '-m', full_message], capture_output=True, text=True)
                        
                        if result.returncode == 0:
                            print("‚úÖ Commit criado")
                            
                            # Push com retry
                            for attempt in range(3):
                                push_result = subprocess.run(['git', 'push', 'origin', branch], capture_output=True, text=True)
                                if push_result.returncode == 0:
                                    print("‚úÖ Push realizado com sucesso")
                                    break
                                else:
                                    print(f"‚ö†Ô∏è Tentativa {attempt + 1} de push falhou: {push_result.stderr}")
                                    if attempt < 2:
                                        time.sleep(5)
                    else:
                        print("‚ÑπÔ∏è Nenhuma mudan√ßa para commitar")
            
            # Salvar estado atualizado
            print("\nüíæ Salvando estado da sincroniza√ß√£o...")
            
            # Fazer backup do estado anterior
            if os.path.exists(SYNC_STATE_FILE):
                with open(SYNC_STATE_FILE, 'r') as f:
                    backup_content = f.read()
                with open(f"{SYNC_STATE_FILE}.backup", 'w') as f:
                    f.write(backup_content)
            
            # Recarregar arquivos ap√≥s sincroniza√ß√£o
            current_local = get_local_files() if SYNC_MODE != 'drive-to-github' else {}
            current_drive = get_drive_files(repo_folder_id) if SYNC_MODE != 'github-to-drive' else {}
            
            new_state = {
                'files': {},
                'last_sync': datetime.now(timezone.utc).isoformat(),
                'sync_mode': SYNC_MODE,
                'stats': {
                    'downloaded': counters['downloaded'],
                    'uploaded': counters['uploaded'],
                    'errors': counters['errors'],
                    'total_files': len(current_local) + len(current_drive)
                }
            }
            
            # Salvar hashes de todos os arquivos
            for path in set(current_local.keys()) | set(current_drive.keys()):
                new_state['files'][path] = {
                    'local_hash': current_local.get(path, {}).get('hash'),
                    'drive_hash': current_drive.get(path, {}).get('hash'),
                    'local_size': current_local.get(path, {}).get('size'),
                    'drive_size': current_drive.get(path, {}).get('size'),
                    'local_modified': current_local.get(path, {}).get('modified'),
                    'drive_modified': current_drive.get(path, {}).get('modified')
                }
            
            save_sync_state(new_state)
            
            # Atualizar hist√≥rico
            if total_changes > 0:
                print("\nüìã Atualizando hist√≥rico...")
                
                history_entry = f"\n## üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')} [{SYNC_MODE}]\n\n"
                history_entry += f"**Modo:** {SYNC_MODE}\n"
                history_entry += f"**Dura√ß√£o:** {elapsed_time/60:.1f} minutos\n\n"
                history_entry += f"**Estat√≠sticas:**\n"
                history_entry += f"- Total de arquivos: {new_state['stats']['total_files']}\n"
                history_entry += f"- Downloads: {counters['downloaded']} sucesso, {counters['errors']} erros\n"
                history_entry += f"- Uploads: {counters['uploaded']} sucesso\n"
                history_entry += f"- Deletados: {len(changes['delete_from_drive']) + len(changes['delete_from_local'])}\n"
                
                if total_changes <= 50:
                    history_entry += f"\n**Opera√ß√µes executadas:**\n"
                    
                    if changes['local_to_drive']:
                        history_entry += f"\n‚¨ÜÔ∏è Enviados para o Drive:\n"
                        for path in sorted(changes['local_to_drive'])[:10]:
                            history_entry += f"- {path}\n"
                        if len(changes['local_to_drive']) > 10:
                            history_entry += f"... e mais {len(changes['local_to_drive']) - 10} arquivo(s)\n"
                    
                    if changes['drive_to_local']:
                        history_entry += f"\n‚¨áÔ∏è Baixados do Drive:\n"
                        for path in sorted(changes['drive_to_local'])[:10]:
                            history_entry += f"- {path}\n"
                        if len(changes['drive_to_local']) > 10:
                            history_entry += f"... e mais {len(changes['drive_to_local']) - 10} arquivo(s)\n"
                
                history_entry += "\n---\n"
                
                # Adicionar ao arquivo
                if os.path.exists(SYNC_HISTORY_FILE):
                    with open(SYNC_HISTORY_FILE, 'r') as f:
                        current_history = f.read()
                else:
                    current_history = "# üîÑ Hist√≥rico de Sincroniza√ß√£o\n\nSincroniza√ß√£o autom√°tica entre GitHub e Google Drive.\n\n---\n"
                
                with open(SYNC_HISTORY_FILE, 'w') as f:
                    f.write(current_history + history_entry)
                
                # Adicionar hist√≥rico ao commit se necess√°rio
                subprocess.run(['git', 'add', SYNC_HISTORY_FILE], capture_output=True)
                subprocess.run(['git', 'add', SYNC_STATE_FILE], capture_output=True)
                subprocess.run(['git', 'commit', '--amend', '--no-edit'], capture_output=True)
            
            print("\n‚ú® Sincroniza√ß√£o finalizada com sucesso!")
            print(f"üìä Resumo: {counters['downloaded']} downloads, {counters['uploaded']} uploads, {counters['errors']} erros")
            
        except Exception as e:
            print(f"\n‚ùå Erro fatal na sincroniza√ß√£o: {e}")
            traceback.print_exc()
            sys.exit(1)
            
        finally:
            # Remover lock
            if os.path.exists(SYNC_LOCK_FILE):
                os.remove(SYNC_LOCK_FILE)
                print("üîì Lock removido")
        
        EOF
