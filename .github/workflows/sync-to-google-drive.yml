name: Optimized PDF Sync to Google Drive

on:
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  sync-pdfs:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 horas para repositórios grandes
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install Dependencies
        run: |
          pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2
          pip install requests tqdm
      
      - name: Count and Categorize Files
        id: file-analysis
        run: |
          pdf_count=$(find . -name "*.pdf" -type f | wc -l)
          total_size=$(find . -name "*.pdf" -type f -exec du -b {} + | awk '{sum+=$1} END {print sum}')
          
          echo "PDF_COUNT=$pdf_count" >> $GITHUB_OUTPUT
          echo "TOTAL_SIZE_MB=$((total_size / 1024 / 1024))" >> $GITHUB_OUTPUT
          
          echo "📊 Análise do Repositório:"
          echo "- Arquivos PDF encontrados: $pdf_count"
          echo "- Tamanho total dos PDFs: $((total_size / 1024 / 1024)) MB"
      
      - name: Optimized PDF Sync
        timeout-minutes: 150
        env:
          GOOGLE_CLIENT_ID: ${{ secrets.GOOGLE_CLIENT_ID }}
          GOOGLE_CLIENT_SECRET: ${{ secrets.GOOGLE_CLIENT_SECRET }}
          GOOGLE_REFRESH_TOKEN: ${{ secrets.GOOGLE_REFRESH_TOKEN }}
          GOOGLE_DRIVE_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
        run: |
          python3 << 'EOF'
          import os
          import time
          import hashlib
          import json
          from pathlib import Path
          from concurrent.futures import ThreadPoolExecutor, as_completed
          from threading import Semaphore
          import random
          
          from google.oauth2.credentials import Credentials
          from google.auth.transport.requests import Request
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload
          from googleapiclient.errors import HttpError
          
          # Configuração de rate limiting
          UPLOAD_SEMAPHORE = Semaphore(3)  # Máximo 3 uploads simultâneos
          REQUEST_DELAY = 1.2  # Delay entre requests (1.2s para ficar abaixo do limite)
          
          class OptimizedPDFSync:
              def __init__(self):
                  self.setup_credentials()
                  self.drive_service = build('drive', 'v3', credentials=self.creds)
                  self.folder_id = os.environ['GOOGLE_DRIVE_FOLDER_ID']
                  
              def setup_credentials(self):
                  """Setup OAuth2 credentials from environment"""
                  self.creds = Credentials(
                      token=None,
                      refresh_token=os.environ['GOOGLE_REFRESH_TOKEN'],
                      token_uri='https://oauth2.googleapis.com/token',
                      client_id=os.environ['GOOGLE_CLIENT_ID'],
                      client_secret=os.environ['GOOGLE_CLIENT_SECRET']
                  )
                  self.creds.refresh(Request())
              
              def get_file_hash(self, filepath):
                  """Gera hash MD5 do arquivo PDF"""
                  hash_md5 = hashlib.md5()
                  with open(filepath, "rb") as f:
                      for chunk in iter(lambda: f.read(8192), b""):
                          hash_md5.update(chunk)
                  return hash_md5.hexdigest()
              
              def check_existing_file(self, filename, file_hash):
                  """Verifica se arquivo já existe no Drive com mesmo hash"""
                  try:
                      query = f"name='{filename}' and parents in '{self.folder_id}' and trashed=false"
                      results = self.drive_service.files().list(
                          q=query,
                          fields="files(id,name,properties,md5Checksum)"
                      ).execute()
                      
                      files = results.get('files', [])
                      for file in files:
                          if file.get('md5Checksum') == file_hash:
                              return file['id']
                      return None
                      
                  except HttpError as e:
                      print(f"❌ Erro ao verificar arquivo existente {filename}: {e}")
                      return None
              
              def upload_pdf_with_retry(self, pdf_path, max_retries=5):
                  """Upload PDF com retry exponencial"""
                  filename = os.path.basename(pdf_path)
                  file_hash = self.get_file_hash(pdf_path)
                  
                  # Verificar se já existe
                  existing_id = self.check_existing_file(filename, file_hash)
                  if existing_id:
                      print(f"⏭️  Pulando {filename} (já existe com mesmo hash)")
                      return True
                  
                  # Upload com retry
                  for attempt in range(max_retries):
                      try:
                          with UPLOAD_SEMAPHORE:
                              media = MediaFileUpload(
                                  pdf_path,
                                  mimetype='application/pdf',
                                  resumable=True
                              )
                              
                              file_metadata = {
                                  'name': filename,
                                  'parents': [self.folder_id],
                                  'properties': {
                                      'source': 'github-actions',
                                      'hash': file_hash,
                                      'upload_date': time.strftime('%Y-%m-%d %H:%M:%S')
                                  }
                              }
                              
                              file = self.drive_service.files().create(
                                  body=file_metadata,
                                  media_body=media,
                                  fields='id,name,size'
                              ).execute()
                              
                              size_mb = int(file.get('size', 0)) / 1024 / 1024
                              print(f"✅ Upload concluído: {filename} ({size_mb:.1f} MB)")
                              
                              # Delay para rate limiting
                              time.sleep(REQUEST_DELAY)
                              return True
                              
                      except HttpError as e:
                          if e.resp.status in [429, 403, 500, 502, 503, 504]:
                              wait_time = (2 ** attempt) + random.uniform(0, 2)
                              print(f"⏳ Rate limit atingido para {filename}. Aguardando {wait_time:.1f}s (tentativa {attempt + 1})")
                              time.sleep(wait_time)
                          else:
                              print(f"❌ Erro permanente no upload de {filename}: {e}")
                              return False
                      except Exception as e:
                          print(f"❌ Erro inesperado no upload de {filename}: {e}")
                          return False
                  
                  print(f"❌ Falha no upload de {filename} após {max_retries} tentativas")
                  return False
              
              def sync_pdfs_optimized(self):
                  """Sincronização otimizada de PDFs"""
                  pdf_files = list(Path('.').rglob('*.pdf'))
                  
                  if not pdf_files:
                      print("📝 Nenhum arquivo PDF encontrado no repositório")
                      return
                  
                  print(f"🔄 Iniciando sincronização de {len(pdf_files)} arquivos PDF")
                  
                  # Processar em lotes para controlar rate limits
                  batch_size = 5
                  successful_uploads = 0
                  failed_uploads = 0
                  
                  for i in range(0, len(pdf_files), batch_size):
                      batch = pdf_files[i:i + batch_size]
                      print(f"\n📦 Processando lote {i//batch_size + 1}/{(len(pdf_files) + batch_size - 1)//batch_size}")
                      
                      # Usar ThreadPoolExecutor para paralelização controlada
                      with ThreadPoolExecutor(max_workers=2) as executor:
                          futures = [executor.submit(self.upload_pdf_with_retry, str(pdf)) for pdf in batch]
                          
                          for future in as_completed(futures):
                              if future.result():
                                  successful_uploads += 1
                              else:
                                  failed_uploads += 1
                      
                      # Delay entre lotes para não sobrecarregar a API
                      if i + batch_size < len(pdf_files):
                          print("⏸️  Pausa entre lotes para respeitar rate limits...")
                          time.sleep(10)
                  
                  print(f"\n📊 Resumo da sincronização:")
                  print(f"✅ Uploads bem-sucedidos: {successful_uploads}")
                  print(f"❌ Uploads falharam: {failed_uploads}")
                  print(f"📁 Total de arquivos: {len(pdf_files)}")
          
          # Executar sincronização otimizada
          if __name__ == "__main__":
              sync = OptimizedPDFSync()
              sync.sync_pdfs_optimized()
          EOF
      
      - name: Cleanup and Summary
        if: always()
        run: |
          echo "🏁 Sincronização concluída!"
          echo "📊 Estatísticas finais:"
          echo "- Arquivos PDF processados: ${{ steps.file-analysis.outputs.PDF_COUNT }}"
          echo "- Tamanho total: ${{ steps.file-analysis.outputs.TOTAL_SIZE_MB }} MB"
          echo "- Horário: $(date '+%Y-%m-%d %H:%M:%S UTC')"
