name: Sync Repository to Google Drive (Optimized)

on:
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  sync-to-drive:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 horas m√°ximo
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Dependencies
      run: |
        pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2
        
    - name: Setup Google Credentials
      run: |
        echo "${{ secrets.GOOGLE_CREDENTIALS }}" | base64 -d > credentials.json
        echo "GOOGLE_APPLICATION_CREDENTIALS=credentials.json" >> $GITHUB_ENV
        
    - name: Incremental Sync with Chunking
      run: |
        python << 'EOF'
        import os
        import time
        import hashlib
        from pathlib import Path
        from google.oauth2 import serviceaccount
        from googleapiclient.discovery import build
        from googleapiclient.http import MediaFileUpload
        from googleapiclient.errors import HttpError
        
        # Configura√ß√£o
        credentials = serviceaccount.Credentials.from_service_account_file(
            'credentials.json',
            scopes=['https://www.googleapis.com/auth/drive']
        )
        service = build('drive', 'v3', credentials=credentials)
        
        PARENT_FOLDER_ID = "${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}"
        REPO_NAME = "${{ github.event.repository.name }}"
        MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB por arquivo
        CHUNK_SIZE = 10  # Processar 10 arquivos por vez
        
        def get_file_hash(file_path):
            """Gerar hash MD5 do arquivo para compara√ß√£o"""
            hasher = hashlib.md5()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        
        def find_or_create_folder(name, parent_id):
            """Encontrar ou criar pasta no Google Drive"""
            query = f"name='{name}' and '{parent_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false"
            results = service.files().list(q=query, fields="files(id,name)").execute()
            folders = results.get('files', [])
            
            if folders:
                return folders[0]['id']
            else:
                folder_metadata = {
                    'name': name,
                    'mimeType': 'application/vnd.google-apps.folder',
                    'parents': [parent_id]
                }
                folder = service.files().create(body=folder_metadata, fields='id').execute()
                return folder.get('id')
        
        def upload_file_with_retry(file_path, parent_id, max_retries=3):
            """Upload arquivo com retry autom√°tico"""
            file_name = os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            
            # Pular arquivos muito grandes
            if file_size > MAX_FILE_SIZE:
                print(f"‚ö†Ô∏è Arquivo muito grande (ignorado): {file_name} ({file_size/1024/1024:.1f}MB)")
                return None
            
            # Verificar se arquivo j√° existe
            query = f"name='{file_name}' and '{parent_id}' in parents and trashed=false"
            existing = service.files().list(q=query, fields="files(id,name)").execute()
            
            for attempt in range(max_retries):
                try:
                    file_metadata = {
                        'name': file_name,
                        'parents': [parent_id]
                    }
                    
                    # Determinar MIME type
                    mime_type = 'application/octet-stream'
                    if file_name.endswith('.pdf'):
                        mime_type = 'application/pdf'
                    elif file_name.endswith('.py'):
                        mime_type = 'text/x-python'
                    elif file_name.endswith('.md'):
                        mime_type = 'text/markdown'
                    
                    media = MediaFileUpload(file_path, mimetype=mime_type, resumable=True)
                    
                    if existing.get('files'):
                        # Atualizar arquivo existente
                        file_id = existing['files'][0]['id']
                        file = service.files().update(
                            fileId=file_id,
                            body=file_metadata,
                            media_body=media
                        ).execute()
                        print(f"üìù Atualizado: {file_name}")
                    else:
                        # Criar novo arquivo
                        file = service.files().create(
                            body=file_metadata,
                            media_body=media,
                            fields='id,name'
                        ).execute()
                        print(f"‚úÖ Enviado: {file_name}")
                    
                    return file.get('id')
                    
                except HttpError as e:
                    if e.resp.status == 429:  # Rate limit
                        wait_time = 2 ** attempt
                        print(f"‚è≥ Rate limit - aguardando {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        print(f"‚ùå Erro no upload de {file_name}: {e}")
                        break
                except Exception as e:
                    print(f"‚ùå Erro inesperado em {file_name}: {e}")
                    break
            
            return None
        
        # Criar pasta do reposit√≥rio
        repo_folder_id = find_or_create_folder(REPO_NAME, PARENT_FOLDER_ID)
        print(f"üìÅ Pasta do reposit√≥rio: {REPO_NAME}")
        
        # Coletar todos os arquivos (exceto ignorados)
        all_files = []
        ignore_patterns = {'.git', '__pycache__', 'node_modules', '.github', 'credentials.json'}
        
        for root, dirs, files in os.walk('.'):
            # Filtrar diret√≥rios ignorados
            dirs[:] = [d for d in dirs if d not in ignore_patterns]
            
            for file in files:
                if not any(pattern in file for pattern in ['.pyc', '.tmp']):
                    file_path = os.path.join(root, file)
                    all_files.append(file_path)
        
        # Separar PDFs dos outros arquivos
        pdf_files = [f for f in all_files if f.endswith('.pdf')]
        other_files = [f for f in all_files if not f.endswith('.pdf')]
        
        print(f"üìä Total de arquivos: {len(all_files)}")
        print(f"üìÑ PDFs: {len(pdf_files)}")
        print(f"üìù Outros: {len(other_files)}")
        
        # Processar outros arquivos primeiro (mais r√°pido)
        print("\nüöÄ Processando arquivos n√£o-PDF...")
        for i in range(0, len(other_files), CHUNK_SIZE):
            chunk = other_files[i:i+CHUNK_SIZE]
            print(f"üì¶ Lote {i//CHUNK_SIZE + 1}: {len(chunk)} arquivos")
            
            for file_path in chunk:
                # Criar estrutura de pastas no Drive
                rel_path = os.path.relpath(file_path, '.')
                dir_path = os.path.dirname(rel_path)
                
                current_folder_id = repo_folder_id
                if dir_path and dir_path != '.':
                    for folder_name in dir_path.split(os.sep):
                        current_folder_id = find_or_create_folder(folder_name, current_folder_id)
                
                upload_file_with_retry(file_path, current_folder_id)
            
            time.sleep(1)  # Pausa entre lotes
        
        # Processar PDFs em lotes menores
        print("\nüìÑ Processando arquivos PDF...")
        pdf_chunk_size = 5  # Lotes menores para PDFs
        
        for i in range(0, len(pdf_files), pdf_chunk_size):
            chunk = pdf_files[i:i+pdf_chunk_size]
            print(f"üì¶ Lote PDF {i//pdf_chunk_size + 1}: {len(chunk)} arquivos")
            
            for file_path in chunk:
                rel_path = os.path.relpath(file_path, '.')
                dir_path = os.path.dirname(rel_path)
                
                current_folder_id = repo_folder_id
                if dir_path and dir_path != '.':
                    for folder_name in dir_path.split(os.sep):
                        current_folder_id = find_or_create_folder(folder_name, current_folder_id)
                
                upload_file_with_retry(file_path, current_folder_id)
            
            time.sleep(2)  # Pausa maior entre lotes de PDF
        
        print(f"\nüéâ Sincroniza√ß√£o conclu√≠da!")
        print(f"üìÅ Reposit√≥rio sincronizado em: Sistema/GAD/RepositoriosGithubGad/{REPO_NAME}")
        EOF
